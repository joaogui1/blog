<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.68.3" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Uma introduação a JAX &middot; Programmer in Progress</title>
  <meta name="description" content="" />

  
  <link type="text/css" rel="stylesheet" href="/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="/css/poole.css">
  <link type="text/css" rel="stylesheet" href="/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="/"><h1>Programmer in Progress</h1></a>
      <p class="lead">
       My description 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="/">Home</a> </li>
        <li><a href="/page/about"> About </a></li><li><a href="/post/"> Blog </a></li><li><a href="https://github.com/joaogui1"> Github </a></li><li><a href="/page/resources"> Resources </a></li>
      </ul>
    </nav>

    <p>&copy; 2020. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>Uma introduação a JAX</h1>
  <time datetime=2020-05-25T00:00:00Z class="post-date">Mon, May 25, 2020</time>
  <p><a href="https://github.com/google/jax">JAX</a> é uma nova biblioteca para Python da Google com foco em pesquisa de alta performance em Aprendizado de Máquina e seguindo o paradigma de programação funcional
Mais especificamente JAX nos dá acesso a uma API compatível com numpy e scipy e transformações de função, as principais sendo grad, jit, vmap e pmap(que vai ter seu próprio post no futuro).</p>
<h2 id="o-wrapper-de-numpy-jaxnumpy">O Wrapper de Numpy: jax.numpy</h2>
<p>JAX nos dá acesso ao jax.numpy, uma reinplementação das funções do Numpy que são transformáveis pelas pelas trasformações de função do JAX.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> jax.numpy <span style="color:#f92672">as</span> jnp
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>])
b <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">1.</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>])
<span style="color:#66d9ef">print</span>(np<span style="color:#f92672">.</span>dot(a, b), jnp<span style="color:#f92672">.</span>dot(a, b))
</code></pre></div><pre><code>0.0 0.0
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">(np<span style="color:#f92672">.</span>square(a), jnp<span style="color:#f92672">.</span>square(a))
</code></pre></div><pre><code>(array([1., 4., 9.]), DeviceArray([1., 4., 9.], dtype=float32))
</code></pre>
<p>Note que JAX tem seu próprio tipo de array, o DeviceArray, em geral as funções vão castar arrays de numpy para DeviceArrays, então se você quiser boa performance é melhor fazer esse casting manualmente antes de passar os dados para várias funções.
Uma outra diferença é números aleatórios funcionam. JAX nãp tem a jax.numpy.random, em vez disso ele tem a sua própria sub-biblioteca jax.random</p>
<h2 id="números-aleatórios-jaxrandom">Números aleatórios jax.random</h2>
<p>Uma das partes mais peculiares de JAX, para faciliar implementações usando paralelismo não existe uma semente global para geradores de números aleatórios, em vez disso em JAX você passa explicitamente a seed para cada função que envolve aleatoriedade, e cabe a você atualizá-la</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> jax
key <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>PRNGKey(<span style="color:#ae81ff">42</span>) <span style="color:#75715e">#cria um semente aleatória</span>
a <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(key, ())
b <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(key, ())
<span style="color:#66d9ef">print</span>(a, b)
<span style="color:#66d9ef">print</span>(a <span style="color:#f92672">==</span> b) <span style="color:#75715e">#como usamos a mesma semente para a mesma função temos valores iguais</span>
k1, k2 <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>split(key, <span style="color:#ae81ff">2</span>) <span style="color:#75715e">#vamos criar duas novas seeds a partir da primeira</span>
a <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(k1, ())
b <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(k2, ())
<span style="color:#66d9ef">print</span>(a, b) <span style="color:#75715e">#Agora são diferentes</span>
</code></pre></div><pre><code>-0.18471177 -0.18471177
True
0.13790321 1.3694694
</code></pre>
<h2 id="transformações">Transformações</h2>
<p>O principal diferencial de JAX são suas tranformações de funções, que nos permitem permitem modificar de maneiras bem úteis funções definidas a partir de outras funções do JAX e algumas primitivas de Python. Algo muito útil e legal delas é que elas podem ser utilizadas em conjunto, nos permitindo por exemplo compilar a derivada de uma função vetorizada apenas aplicando 3 transformações uma seguida da outra. Porém existem alguns cuidados que dever ser tomados ao se usar esses transformações, para entender esse cuidados melhores cheque esse <a href="https://github.com/google/jax#current-gotchas">link</a> e abra o notebook</p>
<h3 id="diferenciação-automática-jaxgrad">Diferenciação Automática: jax.grad</h3>
<p>Em aprendizado de máquina, principalmente quando estamos tratando de redes neurais, lidamos com muitas derivas, gradientes e afins: Para treinar uma regressão linear ou logística, precisamos computar um hessiano, para treinar uma rede neural usamos descida de gradiente, que requer o cálculo de um gradiente, dentre outros exemplos.
Computar essas derivadas na mão é muitas vezes impossível (por questão de tempo), assim temos algoritmos como o backpropagation para redes neurais, porém se sempre tivessemos que implementar nós mesmos esse algoritmo, e implementar a derivada de cada uma das funções que vamos usar, terminaríamos com uma quatidade imensa de código duplicado, além duma imensa chance de errarmos algo na implementação e terminarmos sem conseguir bons resultados ou com resultados que não correspodem a realidade.
Para lidar com isso temos diferenciação automática, simplesmente ter diferenciação automática para as funções de Numpy já é o bastante para uma biblioteca mostrar seu valor, e no caso existe uma biblioteca que é exatamente isso, chamada de Autograd, em muitos sentidos JAX é um sucessor dessa biblioteca, inclusive ambas têm muitos desenvolvedores em comum.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> jax <span style="color:#f92672">import</span> grad
<span style="color:#f92672">from</span> math <span style="color:#f92672">import</span> pi, sqrt
dup <span style="color:#f92672">=</span> grad(jnp<span style="color:#f92672">.</span>square)
<span style="color:#66d9ef">print</span>(dup(<span style="color:#ae81ff">3.0</span>)) <span style="color:#75715e">#A derivada de x² é 2x</span>
<span style="color:#66d9ef">print</span>(grad(dup)(<span style="color:#ae81ff">3.0</span>)) <span style="color:#75715e">#Podemos aplicar várias vezes a grad</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">composite_func</span>(x):
    y <span style="color:#f92672">=</span> x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>
    <span style="color:#66d9ef">return</span> jnp<span style="color:#f92672">.</span>cos(y)

g <span style="color:#f92672">=</span> grad(composite_func) <span style="color:#75715e"># Pela regra da cadeia, dcos(x²)/dx = -2xsen(x²)</span>
<span style="color:#66d9ef">print</span>(g(jnp<span style="color:#f92672">.</span>sqrt(pi<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>)), <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>sqrt(pi<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>))
</code></pre></div><pre><code>6.0
2.0
-2.5066283 -2.5066282746310002
</code></pre>
<p>Para funções com várias variáveis de entrada a grad por padrão nos dá a derivada em função do primeiro parâmetro, mas podemos mudar isso com o argumento argnums. Também vale ressaltar que os argumentos não precisam ser apenas números e pode ser vetores</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">f</span>(x, y):
    <span style="color:#66d9ef">return</span> x<span style="color:#f92672">*</span>(y<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
dfdy <span style="color:#f92672">=</span> grad(f, argnums<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>))
<span style="color:#66d9ef">print</span>(dfdy(<span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">4.0</span>))
gradient <span style="color:#f92672">=</span> grad(f, argnums<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>))
<span style="color:#66d9ef">print</span>(gradient(<span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">4.0</span>))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">g</span>(v):
    <span style="color:#66d9ef">return</span> jnp<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>norm(v)
<span style="color:#66d9ef">print</span>(grad(g)(a))
</code></pre></div><pre><code>24.0
(DeviceArray(16., dtype=float32), DeviceArray(24., dtype=float32))
1.0
</code></pre>
<h3 id="compilação-com-xla-jit">Compilação com XLA: jit</h3>
<p>Mas as vantagens de jax não param em diferenciação automática, se não seria apenas um clone do autograd, jax também tem a habilidade de compilar funções usando o XLA (accelerated linear algebra) da Google, tornando-as bem mais rápidas, além de permitir o uso de aceleradores como GPUs e TPUs.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dot <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>jit(jnp<span style="color:#f92672">.</span>dot)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">a <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(k1, (<span style="color:#ae81ff">2024</span>, <span style="color:#ae81ff">2024</span>))
b <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(k2, (<span style="color:#ae81ff">2024</span>, <span style="color:#ae81ff">2024</span>))
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%%</span>timeit
np<span style="color:#f92672">.</span>dot(a, b)
</code></pre></div><pre><code>1 loop, best of 3: 256 ms per loop
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%%</span>timeit
dot(a, b)
</code></pre></div><pre><code>The slowest run took 1255.15 times longer than the fastest. This could mean that an intermediate result is being cached.
1 loop, best of 3: 169 µs per loop
</code></pre>
<h2 id="vetorização-automática-vmap">Vetorização Automática: vmap</h2>
<p>Vmap é uma transformação muito interessante, usando ela é possível vetorizar automaticamente nossas funções, ou seja, em vez de ter que fazer uma função que lida com um batch de dados, podemos fazer uma função que recebe um único dado e depois usar a trasformação para ganhar a versão que lida com o batch.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>])
b <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">1.</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>])
c <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">3.</span>], [<span style="color:#ae81ff">4.</span>, <span style="color:#ae81ff">5.</span>, <span style="color:#ae81ff">6.</span>]])

<span style="color:#a6e22e">@jax.vmap</span> <span style="color:#75715e">#Podemos usar as transformações como decoradores</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">f</span>(x, y):
    <span style="color:#66d9ef">return</span> x<span style="color:#f92672">/</span>y <span style="color:#f92672">+</span> <span style="color:#ae81ff">1.</span>
<span style="color:#66d9ef">print</span>(f(a, b))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prod</span>(x, y):
    <span style="color:#66d9ef">return</span> x<span style="color:#a6e22e">@y</span>
<span style="color:#66d9ef">print</span>(prod(a, b))
</code></pre></div><pre><code>[ 2.  3. -2.]
0.0
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">prod(a, c) <span style="color:#75715e">#a e c não têm dimensões compatíveis</span>
</code></pre></div><pre><code>---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-11-908e821a683b&gt; in &lt;module&gt;()
----&gt; 1 prod(a, c) #a e c não têm dimensões compatíveis


&lt;ipython-input-10-7f441aca0238&gt; in prod(x, y)
      9 
     10 def prod(x, y):
---&gt; 11     return x@y
     12 print(prod(a, b))


ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3)
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">batch_prod <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>vmap(prod, in_axes<span style="color:#f92672">=</span>(None, <span style="color:#ae81ff">0</span>)) <span style="color:#75715e">#vamos multiplica a por cada linha de c</span>
batch_prod(a, c)
</code></pre></div><pre><code>DeviceArray([14., 32.], dtype=float32)
</code></pre>
<p>Nessa primeira parte vimos qual o propósito da biblioteca e suas principais funções, nos próximos posts vamos explorar como criar redes neurais com jax, suas bibliotecas experimentais, o ecossistema de bibliotecas escritas usando jax e a pmap</p>

</div>


    </main>

    
      
    
  </body>
</html>
