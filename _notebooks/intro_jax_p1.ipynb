{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "intro_jax.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1N5C6V8ZtBi",
        "colab_type": "text"
      },
      "source": [
        "# Uma introdução a JAX: Parte 1\n",
        "> Numpy + autograd + XLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3MQ7ccBZtBj",
        "colab_type": "text"
      },
      "source": [
        "[JAX](https://github.com/google/jax) é uma nova biblioteca para Python da Google com foco em pesquisa de alta performance em Aprendizado de Máquina e seguindo o paradigma de programação funcional \n",
        "Mais especificamente JAX nos dá acesso a uma API compatível com numpy e scipy e transformações de função, as principais sendo grad, jit, vmap e pmap(que vai ter seu próprio post no futuro)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p02pFSKkZtBk",
        "colab_type": "text"
      },
      "source": [
        "## O Wrapper de Numpy: jax.numpy\n",
        "JAX nos dá acesso ao jax.numpy, uma reinplementação das funções do Numpy que são transformáveis pelas pelas trasformações de função do JAX."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOLqIMhgZtBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8388ead1-aec1-430d-ade1-261d78f3f204"
      },
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([1., 2., 3.])\n",
        "b = np.array([1., 1., -1.])\n",
        "print(np.dot(a, b), jnp.dot(a, b))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oFL5_B5ZtBp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38a87fcb-3e49-45a4-e89b-614e024aad38"
      },
      "source": [
        "(np.square(a), jnp.square(a))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1., 4., 9.]), DeviceArray([1., 4., 9.], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBo2cXoKZtBt",
        "colab_type": "text"
      },
      "source": [
        "Note que JAX tem seu próprio tipo de array, o DeviceArray, em geral as funções vão castar arrays de numpy para DeviceArrays, então se você quiser boa performance é melhor fazer esse casting manualmente antes de passar os dados para várias funções.\n",
        "Uma outra diferença é números aleatórios funcionam. JAX nãp tem a jax.numpy.random, em vez disso ele tem a sua própria sub-biblioteca jax.random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYkFgBzmZtBt",
        "colab_type": "text"
      },
      "source": [
        "## Números aleatórios jax.random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA7Z6HCqZtBu",
        "colab_type": "text"
      },
      "source": [
        "Uma das partes mais peculiares de JAX, para faciliar implementações usando paralelismo não existe uma semente global para geradores de números aleatórios, em vez disso em JAX você passa explicitamente a seed para cada função que envolve aleatoriedade, e cabe a você atualizá-la"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kduqEFsyZtBu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bae5c980-0380-431a-c404-529ce03b7264"
      },
      "source": [
        "import jax\n",
        "key = jax.random.PRNGKey(42) #cria um semente aleatória\n",
        "a = jax.random.normal(key, ())\n",
        "b = jax.random.normal(key, ())\n",
        "print(a, b)\n",
        "print(a == b) #como usamos a mesma semente para a mesma função temos valores iguais\n",
        "k1, k2 = jax.random.split(key, 2) #vamos criar duas novas seeds a partir da primeira\n",
        "a = jax.random.normal(k1, ())\n",
        "b = jax.random.normal(k2, ())\n",
        "print(a, b) #Agora são diferentes"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.18471177 -0.18471177\n",
            "True\n",
            "0.13790321 1.3694694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXhl5rSWZtBx",
        "colab_type": "text"
      },
      "source": [
        "## Transformações"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e41V8ZPgZtBx",
        "colab_type": "text"
      },
      "source": [
        "O principal diferencial de JAX são suas tranformações de funções, que nos permitem permitem modificar de maneiras bem úteis funções definidas a partir de outras funções do JAX e algumas primitivas de Python. Algo muito útil e legal delas é que elas podem ser utilizadas em conjunto, nos permitindo por exemplo compilar a derivada de uma função vetorizada apenas aplicando 3 transformações uma seguida da outra. Porém existem alguns cuidados que dever ser tomados ao se usar esses transformações, para entender esse cuidados melhores cheque esse [link](https://github.com/google/jax#current-gotchas) e abra o notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oJOrTu5ZtBy",
        "colab_type": "text"
      },
      "source": [
        "### Diferenciação Automática: jax.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYgAjLUrZtBz",
        "colab_type": "text"
      },
      "source": [
        "Em aprendizado de máquina, principalmente quando estamos tratando de redes neurais, lidamos com muitas derivas, gradientes e afins: Para treinar uma regressão linear ou logística, precisamos computar um hessiano, para treinar uma rede neural usamos descida de gradiente, que requer o cálculo de um gradiente, dentre outros exemplos. \n",
        "Computar essas derivadas na mão é muitas vezes impossível (por questão de tempo), assim temos algoritmos como o backpropagation para redes neurais, porém se sempre tivessemos que implementar nós mesmos esse algoritmo, e implementar a derivada de cada uma das funções que vamos usar, terminaríamos com uma quatidade imensa de código duplicado, além duma imensa chance de errarmos algo na implementação e terminarmos sem conseguir bons resultados ou com resultados que não correspodem a realidade. \n",
        "Para lidar com isso temos diferenciação automática, simplesmente ter diferenciação automática para as funções de Numpy já é o bastante para uma biblioteca mostrar seu valor, e no caso existe uma biblioteca que é exatamente isso, chamada de Autograd, em muitos sentidos JAX é um sucessor dessa biblioteca, inclusive ambas têm muitos desenvolvedores em comum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeOaZKIlZtBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "80961b4a-b8fb-454e-ea5a-7c50ad64bb11"
      },
      "source": [
        "from jax import grad\n",
        "from math import pi, sqrt\n",
        "dup = grad(jnp.square)\n",
        "print(dup(3.0)) #A derivada de x² é 2x\n",
        "print(grad(dup)(3.0)) #Podemos aplicar várias vezes a grad\n",
        "\n",
        "def composite_func(x):\n",
        "    y = x**2\n",
        "    return jnp.cos(y)\n",
        "\n",
        "g = grad(composite_func) # Pela regra da cadeia, dcos(x²)/dx = -2xsen(x²)\n",
        "print(g(jnp.sqrt(pi/2)), -2*sqrt(pi/2))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.0\n",
            "2.0\n",
            "-2.5066283 -2.5066282746310002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eu1m0UJZtB2",
        "colab_type": "text"
      },
      "source": [
        "Para funções com várias variáveis de entrada a grad por padrão nos dá a derivada em função do primeiro parâmetro, mas podemos mudar isso com o argumento argnums. Também vale ressaltar que os argumentos não precisam ser apenas números e pode ser vetores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n-2baR3ZtB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "44719345-257b-4760-844b-193b38c08cde"
      },
      "source": [
        "def f(x, y):\n",
        "    return x*(y**2)\n",
        "dfdy = grad(f, argnums=(1))\n",
        "print(dfdy(3.0, 4.0))\n",
        "gradient = grad(f, argnums=(0, 1))\n",
        "print(gradient(3.0, 4.0))\n",
        "\n",
        "def g(v):\n",
        "    return jnp.linalg.norm(v)\n",
        "print(grad(g)(a))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24.0\n",
            "(DeviceArray(16., dtype=float32), DeviceArray(24., dtype=float32))\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdKB0h_wZtB6",
        "colab_type": "text"
      },
      "source": [
        "### Compilação com XLA: jit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLLEqwVEZtB7",
        "colab_type": "text"
      },
      "source": [
        "Mas as vantagens de jax não param em diferenciação automática, se não seria apenas um clone do autograd, jax também tem a habilidade de compilar funções usando o XLA (accelerated linear algebra) da Google, tornando-as bem mais rápidas, além de permitir o uso de aceleradores como GPUs e TPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jAnNj33ZtB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dot = jax.jit(jnp.dot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOFkOKKIZtB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = jax.random.normal(k1, (2024, 2024))\n",
        "b = jax.random.normal(k2, (2024, 2024))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxTjXIAUZtCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfba9ccc-0dde-48af-a479-8ddeb374a5f1"
      },
      "source": [
        "%%timeit\n",
        "np.dot(a, b)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 256 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2u81FsGZtCC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6ea3b959-1003-4d9f-8a54-6c5e84c1d619"
      },
      "source": [
        "%%timeit\n",
        "dot(a, b)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 1255.15 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 169 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VV1kheEZtCF",
        "colab_type": "text"
      },
      "source": [
        "## Vetorização Automática: vmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxv7jj4lZtCF",
        "colab_type": "text"
      },
      "source": [
        "Vmap é uma transformação muito interessante, usando ela é possível vetorizar automaticamente nossas funções, ou seja, em vez de ter que fazer uma função que lida com um batch de dados, podemos fazer uma função que recebe um único dado e depois usar a trasformação para ganhar a versão que lida com o batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RJZ7RaWZtCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "588ab13c-d6d9-474d-b946-3c82dff9de3e"
      },
      "source": [
        "a = np.array([1., 2., 3.])\n",
        "b = np.array([1., 1., -1.])\n",
        "c = np.array([[1., 2., 3.], [4., 5., 6.]])\n",
        "\n",
        "@jax.vmap #Podemos usar as transformações como decoradores\n",
        "def f(x, y):\n",
        "    return x/y + 1.\n",
        "print(f(a, b))\n",
        "\n",
        "def prod(x, y):\n",
        "    return x@y\n",
        "print(prod(a, b))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.  3. -2.]\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18ZsYMi2ZtCJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "da56c05a-4c88-4b37-b93b-4fcfecda3c62"
      },
      "source": [
        "prod(a, c) #a e c não têm dimensões compatíveis"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-908e821a683b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#a e c não têm dimensões compatíveis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-7f441aca0238>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3ifOMqWZtCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e292c853-aa63-47be-a650-bfa77512de3b"
      },
      "source": [
        "batch_prod = jax.vmap(prod, in_axes=(None, 0)) #vamos multiplica a por cada linha de c\n",
        "batch_prod(a, c)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([14., 32.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_oU1wbnZtCO",
        "colab_type": "text"
      },
      "source": [
        "Nessa primeira parte vimos qual o propósito da biblioteca e suas principais funções, nos próximos posts vamos explorar como criar redes neurais com jax, suas bibliotecas experimentais, o ecossistema de bibliotecas escritas usando jax e a pmap"
      ]
    }
  ]
}